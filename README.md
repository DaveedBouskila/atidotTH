# atidotTH
# **Customer Churn Classification**

## **ğŸ“Œ Overview**
This project builds a robust **churn prediction model** using **time-series data**. The model classifies whether a customer will **churn** between **Jan 1, 2024 â€“ Feb 28, 2024**, based on their transaction history.

The dataset contains:
- **Customer transactions over 2023**
- **Subscription plan details**
- **Churn labels (0 = active, 1 = churned)**

ğŸš€ **Key Features of this Project:**
âœ” Advanced **feature engineering** (inactivity, spending trends, plan risk, etc.)  
âœ” **XGBoost model** with class balancing & recall tuning  
âœ” **SHAP Explainability** to understand key churn factors  
âœ” Clean & optimized **Python code in `DavidBouskilaAtidot.py`**

---

## **ğŸ›  Setup & Installation**
### **1ï¸âƒ£ Install Dependencies**
Ensure you have Python 3.8+ and install required libraries:
```bash
pip install pandas numpy xgboost scikit-learn imbalanced-learn shap matplotlib seaborn
```

### **2ï¸âƒ£ Run the Model**
Simply execute the `DavidBouskilaAtidot.py` script:
```bash
python DavidBouskilaAtidot.py
```
This will:
- **Load and preprocess the dataset** (`churn_data.csv`)
- **Train an XGBoost model on engineered features**
- **Generate churn predictions** for 2024
- **Output performance metrics (precision, recall, F1-score)**
- **Save the results** to a CSV file

---

## **ğŸ“Š Feature Engineering**
To improve model accuracy, the following features were engineered:
1ï¸âƒ£ **`normalized_inactivity`** â†’ Time since last transaction, adjusted for tenure  
2ï¸âƒ£ **`spending_change`** â†’ Rolling spending trends (helps detect declining users)  
3ï¸âƒ£ **`standard_plan_risk`** â†’ Higher risk for Standard plan users based on inactivity  

ğŸ’¡ **Why?** These features were identified using **SHAP analysis** to **increase recall & precision**.

---

## **ğŸš€ Model Training & Evaluation**
The model used **XGBoost**, tuned for **imbalanced data**:
- **`scale_pos_weight=6`** â†’ Adjusts for fewer churn cases
- **Lowered prediction threshold** â†’ Improves recall
- **SMOTE oversampling** â†’ Boosts churn class representation

After training, the model achieved:
```bash
Precision: 51.2%
Recall: 68.8%
F1-score: 58.7%
```
(Exact values depend on dataset variability.)

---

## **ğŸ“‚ Output Files**
- **`churn_predictions.csv`** â†’ Predictions for each customer (0 = active, 1 = churned)
- **`metrics.json`** â†’ Model performance metrics (precision, recall, F1-score)
- **`churn_model.pkl`** â†’ Trained XGBoost model (for deployment)

---

## **ğŸ“Œ Next Steps & Enhancements**
ğŸ”¹ **Test LightGBM or Transformer-based models** (e.g., Temporal Fusion Transformer)  
ğŸ”¹ **Use external data** (e.g., economic trends, seasonality)  
ğŸ”¹ **Fine-tune feature importance thresholds** based on SHAP analysis  

ğŸ“© **Questions?** Feel free to reach out! ğŸš€

